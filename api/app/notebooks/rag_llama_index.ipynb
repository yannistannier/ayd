{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd \n",
    "import llama_index\n",
    "from openai import AzureOpenAI\n",
    "import qdrant_client\n",
    "from typing import Any, List\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.core.bridge.pydantic import PrivateAttr\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    RetrieverEvaluator\n",
    ")\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"no_proxy\"] = \"10.156.254.10\"\n",
    "import openai\n",
    "openai.api_key = \"dtnumds\"\n",
    "openai.api_base = \"http://10.156.254.10:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"dtnumds\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://10.156.254.10:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI( api_key=\"dtnumds\",\n",
    "                azure_endpoint=\"http://10.156.254.10:8000/v1\",\n",
    "                api_version = \"2023-07-01-preview\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in client.models.list().data :\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Tu es un assistant intelligent <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Donne moi des idées de voyages en europe<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.completions.create(prompt = prompt, model=\"llama3-70b\", max_tokens=2048, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(client.embeddings.create(input=\"test\", model=\"dgfip-e5-large\").data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGFIPEmbeddings(BaseEmbedding):\n",
    "    _model_name: str = PrivateAttr()\n",
    "    _openai_client = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_client,\n",
    "        model_name: str = \"dgfip-e5-large\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self._model_name = model_name\n",
    "        self._openai_client = openai_client\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"DGFIPEmbedding\"\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        embeddings = self._openai_client.embeddings.create(\n",
    "            input = query,\n",
    "            model= self._model_name # model = \"deployment_name\".\n",
    "        )\n",
    "        return embeddings.data[0].embedding\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        embeddings = self._openai_client.embeddings.create(\n",
    "            input = text,\n",
    "            model= self._model_name # model = \"deployment_name\".\n",
    "        )\n",
    "        return embeddings.data[0].embedding\n",
    "    \n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self._openai_client.embeddings.create(\n",
    "            input = texts,\n",
    "            model= self._model_name # model = \"deployment_name\".\n",
    "        )\n",
    "        embs = [e.embedding for e in embeddings.data]\n",
    "        return embs\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = DGFIPEmbeddings(openai_client = client, model_name=\"dgfip-e5-large\")\n",
    "Settings.llm  = OpenAILike(model='mixtral-instruct', max_tokens=2048, timeout=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../../../MoDR/data/preprocess/baco_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_folder(x):\n",
    "    return x.split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['folder'] = df['file_name'].apply(extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['folder']==\"bacorh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en place d'un pipeline simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baco_text = df.file_content.to_list()\n",
    "baco_filename = df.file_name.to_list()\n",
    "baco_section = df.section.to_list()\n",
    "baco_question = df.question.to_list()\n",
    "baco_title = df.title.to_list()\n",
    "baco_n_words = df.n_words.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(text=baco_text[i], metadata={\"filename\": baco_filename[i],\n",
    "                                                   \"section\" : baco_section[i],\n",
    "                                                \"question\" : baco_question[i],\n",
    "                                                \"title\": baco_title[i],\n",
    "                                                \"n_words\": baco_n_words[i]\n",
    "                                                    }) for i in range(len(baco_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client_qdrant = qdrant_client.QdrantClient(\n",
    "    \"10.156.254.10:6335\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(client=client_qdrant, collection_name=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import ExactMatchFilter, MetadataFilters, MetadataFilter\n",
    "from llama_index.core.vector_stores import FilterOperator, FilterCondition\n",
    "filters = {\n",
    "    \"index\":\"6669965c92c1832f69f4e931\"\n",
    "}\n",
    "llama_index_filters =  MetadataFilters(filters=[MetadataFilter(key=key, value=value) for key, value in filters.items()], condition=FilterCondition.AND )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.retrieve('Quel est le taux de paiement dématérialisé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../../../data/nausicaa_parsed_eval_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.value_counts(subset=['languages'])[\"['fra']\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"test\": [0,0,0,0,1,1,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries = eval_df.eval_question.to_list()\n",
    "eval_response_eid = eval_df.element_id.to_list()\n",
    "eval_text = eval_df.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriver(eval_queries, response_eid, retriever, precision=1):\n",
    "\n",
    "    correct_matches = 0\n",
    "    for i, query in tqdm(enumerate(eval_queries)):\n",
    "        retriever_response_eid = [n.metadata['element_id'] for n in retriever.retrieve(query)]\n",
    "        \n",
    "        # Check if the correct answer ID is among the top-5 re-ranked answers\n",
    "        if response_eid[i] in retriever_response_eid[:precision]:\n",
    "            correct_matches += 1\n",
    "\n",
    "    accuracy = correct_matches / len(eval_queries)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_retriver(eval_queries = eval_queries, response_eid=eval_response_eid, retriever=retriever, precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "train_nodes = [TextNode(id_=id_, text=text) for id_, text in zip(eval_response_eid, eval_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.core import StorageContext\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents,\n",
    "#    storage_context=storage_context,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(filters=llama_index_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize prompt for LangChain\n",
    "rag_prompt = \"\"\"\n",
    "[INST]\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query in French.\n",
    "Query: {query_str}\n",
    "Answer: \n",
    "[/INST]\n",
    "\"\"\"\n",
    "qa_template = PromptTemplate(rag_prompt)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Qu'est que le club des médiateurs internes ? \", \"test\", \"test2\"]\n",
    "answer = []\n",
    "for q in query :\n",
    "    answer.append(query_engine.query(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a.source_nodes for a in answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "L= np.array([9,2,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def node_parser(nodes: List[Any]) -> str:\n",
    "    context = \"\"\n",
    "    for node in nodes:\n",
    "        context += node.text + \"\\n\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.source_nodes[0].metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Nom du fichier :\" + answer.source_nodes[0].metadata['filename'] + \" / Texte :\" + answer.source_nodes[0].text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode d'évaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = pd.read_csv('data/benchmark_bacorh_min.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_question = eval_dataset['question'].to_list()\n",
    "eval_response = eval_dataset['ground_truths'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_eval = FaithfulnessEvaluator()\n",
    "relevancy_eval = RelevancyEvaluator()\n",
    "correctness_eval = CorrectnessEvaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_eval_prompt = \"\"\"\n",
    "[INST]\n",
    "Please tell if a given piece of information is supported by the context.\n",
    "You need to answer with either YES or NO.\n",
    "Answer YES if any of the context supports the information, even if most of the context is unrelated. Some examples are provided below. \n",
    "\n",
    "Information: Apple pie is generally double-crusted.\n",
    "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
    "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
    "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "Answer: YES\n",
    "Information: Apple pies tastes bad.\n",
    "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
    "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
    "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "Answer: NO\n",
    "Information: {query_str}\n",
    "Context: {context_str}\n",
    "Answer: \n",
    "[/INST]\n",
    "\"\"\"\n",
    "faithfulness_refine_prompt = \"\"\"\n",
    "[INST]\n",
    "We want to understand if the following information is present in the context information: {query_str}\n",
    "We have provided an existing YES/NO answer: {existing_answer}\n",
    "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
    "------------\n",
    "{context_msg}\n",
    "------------\n",
    "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "faithfulness_eval_template = PromptTemplate(faithfulness_eval_prompt)\n",
    "faithfulness_refine_template = PromptTemplate(faithfulness_refine_prompt)\n",
    "\n",
    "\n",
    "faithfulness_eval.update_prompts(\n",
    "    {\"eval_template\": faithfulness_eval_template,\n",
    "     \"refine_template\": faithfulness_refine_template}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_eval_prompt = \"\"\"\n",
    "[INST]\n",
    "Your task is to evaluate if the response for the query     is in line with the context information provided.\n",
    "You have two options to answer. Either YES/ NO.\n",
    "Answer - YES, if the response for the query     is in line with context information otherwise NO.\n",
    "Query and Response: \n",
    " {query_str}\n",
    "Context: \n",
    " {context_str}\n",
    "Answer: \n",
    "[/INST]\n",
    "\"\"\"\n",
    "relevancy_refine_prompt = \"\"\"\n",
    "[INST]\n",
    "We want to understand if the following query and response isin line with the context information: \n",
    " {query_str}\n",
    "We have provided an existing YES/NO answer: \n",
    " {existing_answer}\n",
    "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
    "------------\n",
    "{context_msg}\n",
    "------------\n",
    "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "relevancy_eval_template = PromptTemplate(relevancy_eval_prompt)\n",
    "relevancy_refine_template = PromptTemplate(relevancy_refine_prompt)\n",
    "\n",
    "\n",
    "relevancy_eval.update_prompts(\n",
    "    {\"eval_template\": relevancy_eval_template,\n",
    "     \"refine_template\": relevancy_refine_template}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithfulness(eval_query, faithfulness_evaluator, query_engine):\n",
    "    response = query_engine.query(\n",
    "        eval_query\n",
    "    )\n",
    "    eval_res = faithfulness_evaluator.evaluate_response(response=response)\n",
    "    return eval_res.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevancy(eval_query, relevancy_evaluator, query_engine):\n",
    "    response = query_engine.query(\n",
    "        eval_query\n",
    "    )\n",
    "    eval_res = relevancy_evaluator.evaluate_response(response=response, query=eval_query)\n",
    "    return eval_res.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "fres = 0\n",
    "for eq in tqdm(eval_question):\n",
    "    passing = get_faithfulness(eval_query=eq, faithfulness_evaluator=faithfulness_eval, query_engine=query_engine)\n",
    "    if passing:\n",
    "        fres +=1\n",
    "fres = fres/len(eval_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "rres = 0\n",
    "for eq in tqdm(eval_question):\n",
    "    passing = get_relevancy(eval_query=eq, relevancy_evaluator=relevancy_eval, query_engine=query_engine)\n",
    "    if passing:\n",
    "        rres +=1\n",
    "rres = rres/len(eval_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_response = [[a] for a in eval_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    correct = 0\n",
    "    for result in results:\n",
    "        if result.passing:\n",
    "            correct += 1\n",
    "    score = correct / len(results)\n",
    "    print(f\"{key} Score: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = get_eval_results(\"relevancy\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_prompt = \"\"\"\n",
    "[INST]\n",
    "system: \n",
    "You are an expert evaluation system for a question answering chatbot.\n",
    "\n",
    "You are given the following information:\n",
    "- a user query, and\n",
    "- a generated answer\n",
    "\n",
    "You may also be given a reference answer to use for reference in your evaluation.\n",
    "\n",
    "Your job is to judge the relevance and correctness of the generated answer.\n",
    "Output a single score that represents a holistic evaluation.\n",
    "You must return your response in a line with only the score.\n",
    "Do not return answers in any other format.\n",
    "On a separate line provide your reasoning for the score as well.\n",
    "\n",
    "Follow these guidelines for scoring:\n",
    "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
    "- If the generated answer is not relevant to the user query, you should give a score of 1.\n",
    "- If the generated answer is relevant but contains mistakes, you should give a score between 2 and 3.\n",
    "- If the generated answer is relevant and fully correct, you should give a score between 4 and 5.\n",
    "\n",
    "Example Response:\n",
    "4.0\n",
    "The generated answer has the exact same metrics as the reference answer,     but it is not as concise.\n",
    "\n",
    "\n",
    "user: \n",
    "## User Query\n",
    "{query}\n",
    "\n",
    "## Reference Answer\n",
    "{reference_answer}\n",
    "\n",
    "## Generated Answer\n",
    "{generated_answer}\n",
    "\n",
    "assistant: \n",
    "[/INST]\n",
    "\"\"\"\n",
    "correctness_prompt_template = PromptTemplate(correctness_prompt)\n",
    "\n",
    "\n",
    "correctness_eval.update_prompts(\n",
    "    {\"eval_template\": correctness_prompt_template,\n",
    "     }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctness(eval_query, reference, correctness_evaluator, query_engine):\n",
    "    response = query_engine.query(\n",
    "        eval_query\n",
    "    )\n",
    "    eval_res = correctness_evaluator.evaluate(response=str(response), query=eval_query, reference=str(reference))\n",
    "    return eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness = get_correctness(eval_question[1], eval_response[1], correctness_eval, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cres = 0\n",
    "for eq, er in tqdm(zip(eval_question, eval_response)):\n",
    "    score = get_correctness(eq, er, correctness_eval, query_engine).score\n",
    "    cres += score\n",
    "cres = cres/len(eval_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "#This can be loaded from configs or hardcode the path or pass as a\n",
    "# variable to read_template method it is upto you.\n",
    "\n",
    "yaml_dir_path = \"data.yaml\"\n",
    "\n",
    "def read_template(yaml_dir_path):\n",
    "    '''\n",
    "        This method will read the yaml file from your dir path\n",
    "    '''\n",
    "    directory_path = yaml_dir_path\n",
    "    yaml_content = ''\n",
    "\n",
    "    with open(directory_path, \"r\") as f:\n",
    "        try:\n",
    "            yaml_content = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error parsing {directory_path}: {e}\")\n",
    "    \n",
    "    return yaml_content\n",
    "\n",
    "def get_prompt(yaml_dir_path, task, subtask):\n",
    "    \"\"\"\n",
    "    This method will return you the prompt for the given task\n",
    "\n",
    "    input:\n",
    "    task (str): name of the task like intent, summary, topic discovery etc\n",
    "    version (int): version of the prompt\n",
    "    return (str):\n",
    "    prompt\n",
    "    \"\"\"\n",
    "    yaml_content = read_template(yaml_dir_path)\n",
    "    return yaml_content[task][\"prompts\"][subtask][\"prompt\"]\n",
    "\n",
    "    return yaml_content[task]['prompts'][version][\"prompt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = read_template(\"data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_prompt(\"data.yaml\",\"relevancy\", \"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = PromptTemplate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template.format(context_str=\"test\", query_str=\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayd-api",
   "language": "python",
   "name": "ayd-api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
