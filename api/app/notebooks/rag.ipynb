{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.query_pipeline import QueryPipeline, InputComponent, FnComponent\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Dict, Any, List\n",
    "from abc import ABC, abstractmethod\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.vector_stores import FilterCondition, FilterOperator\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    ExactMatchFilter,\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\", \n",
    "    timeout=60,\n",
    "    api_key=\"sk-xxxx\"\n",
    ")\n",
    "llm_model = OpenAI(model=\"gpt-3.5-turbo\",timeout=60,api_key=\"sk-xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    \"http://localhost:6333\"\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"qdr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters={\n",
    "    # Depending on the mode ('collection' or 'file'), index is either the collection_id or the token\n",
    "    \"index\": \"667843591624d2699fdc8ce2\"\n",
    "}\n",
    "\n",
    "llama_index_filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=key, value=value)\n",
    "        for key, value in filters.items()\n",
    "    ],\n",
    "    condition=FilterCondition.AND,\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model,\n",
    "    filters=llama_index_filters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "        Tu es un assistant documentaire. Ta missions est de répondre à des questions à partir uniquement du contexte suivant. Si tu ne connais pas l'information, tu diras que tu ne connais pas la réponse. Ta réponse sera concise. Tu utiliseras uniquement l'information pertiente. \n",
    "        Ta réponse devra obligatoirement être en français, c'est un point très important.\n",
    "        Voici un exemple de réponse :\n",
    "        - L'IA générative se base sur l'architecture transformer développée par Google.\n",
    "        \n",
    "        Contexte : \n",
    "        {context_str}\n",
    "\n",
    "        Question :\n",
    "        {query_str}  \n",
    "\n",
    "        Répond à la question en te basant sur le contexte ci-dessus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_index_filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=key, value=value)\n",
    "        for key, value in filters.items()\n",
    "    ],\n",
    "    condition=FilterCondition.AND,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm_model,\n",
    "    filters=llama_index_filters,\n",
    "    similarity_top_k=5,\n",
    "    streaming=False,\n",
    ")\n",
    "query_engine.update_prompts(\n",
    "            {\n",
    "                \"response_synthesizer:text_qa_template\": PromptTemplate(\n",
    "                    prompt\n",
    "                )\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query_engine.query(\"C'est quoi un AAE ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5edb71fea777f5ba1c31428a417773fd00182f5d45b829a52bf159df63af2b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
