{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "import re\n",
    "import openai\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"no_proxy\"] = \"10.156.254.10\"\n",
    "openai.api_key = \"dtnumds\"\n",
    "openai.api_base = \"http://10.156.254.10:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"dtnumds\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://10.156.254.10:8000/v1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question generation exploration\n",
    "\n",
    "L'objectif de ce notebook est d'explorer de manière exhaustive les possibilités offertes par les LLMs pour créer un dataset d'évaluation de nos méthodes de RAG.\n",
    "\n",
    "Docs :\n",
    "- https://mlflow.org/docs/latest/llms/rag/notebooks/question-generation-retrieval-evaluation.html\n",
    "- https://www.promptingguide.ai/applications/synthetic_rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/2023005993-attachment.pdf.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(text):\n",
    "    try :\n",
    "        question = text.split('?')[0] +\" ?\"\n",
    "    except :\n",
    "        question = \"\"\n",
    "        reponse = \"\"\n",
    "    return question\n",
    "def extract_reponse(text):\n",
    "    try :\n",
    "        reponse = text.split('?')[1]\n",
    "    except :\n",
    "        reponse = \"\"\n",
    "    return reponse\n",
    "df['eval_question'] = df['eval_Q&A'].apply(extract_question)\n",
    "df['eval_reponse'] = df['eval_Q&A'].apply(extract_reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_words'] = df.text.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI( api_key=\"dtnumds\",\n",
    "                azure_endpoint=\"http://10.156.254.10:8000/v1\",\n",
    "                api_version = \"2023-07-01-preview\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in client.models.list().data :\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mixtral-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_mixtral_qa = \"\"\"[INST]\n",
    "Ta mission est de générer une couple de question/réponse en français à partir d'un extrait d'un document. Tu généreras une question spécifique à partir du paragraphe donné. \n",
    "Par exemple : \n",
    "- Quelle est l'information clé dans le paragraphe donné ?\n",
    "\n",
    "La réponse doit utilisé autant d'informations que possible. \n",
    "Si vous n'êtes pas en mesure de répondre, donnez la réponse 'Je ne sais pas'.\n",
    "La réponse doit être informative et comporter au plus 3 phrases.\n",
    "\n",
    "Paragraphe : {content}\n",
    "\n",
    "Génére un unique couple de question/réponse en français à partir du paragraphe ci dessus : \n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_mixtral_q = \"\"\"<s> [INST]\n",
    "Ta mission est de générer une question en français à partir d'un extrait d'un document. Tu généreras une question spécifique à partir du paragraphe donné. \n",
    "Par exemple : \n",
    "- Quelle est l'information clé dans le paragraphe donné ?\n",
    "- Combien d'emplois ont été implantés dans les services relocalisés en 2022 ?\n",
    "[/INST]\n",
    "</s>\n",
    "[INST]\n",
    "Paragraphe :\n",
    "{content}\n",
    "\n",
    "Génère une question en français à partir du paragraphe ci-dessus :\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt_mixtral_a = \"\"\"<s> [INST]\n",
    "Tu es un assistant documentaire. Ta missions est de répondre à des questions à partir uniquement du contexte suivant. Si tu ne connais pas l'information, tu diras que tu ne connais pas la réponse. Ta réponse sera concise. Tu utiliseras uniquement l'information pertiente. \n",
    "Ta réponse devra obligatoirement être en français, c'est un point très important.\n",
    "Voici un exemple de réponse :\n",
    "- L'IA générative se base sur l'architecture transformer développée par Google.\n",
    "[/INST]\n",
    "</s>\n",
    "[INST]\n",
    "Contexte : \n",
    "{context_str}\n",
    "\n",
    "Question :\n",
    "{query_str}  \n",
    "\n",
    "Répond à la question en te basant sur le contexte ci-dessus\n",
    "[/INST]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_llama = \"\"\"\n",
    "### System :\n",
    "Your task is to generate question and answer in French based on document.\n",
    "### User :\n",
    "Please generate a question in French asking for the key information in the given paragraph.\n",
    "Also answer the questions using the information in the given paragraph.\n",
    "Please ask the specific question instead of the general question, like\n",
    "'What is the key information in the given paragraph?'.\n",
    "Please generate the answer using as much information as possible.\n",
    "If you are unable to answer it, please generate the answer as 'I don't know.'\n",
    "The answer should be informative and should be more than 3 sentences.\n",
    "\n",
    "Paragraph: {content}\n",
    "\n",
    "### Assistant :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_mixtral_q = [prompt_mixtral_q.format(content = t) for t in texts]\n",
    "#inputs_llama = [prompt_llama.format(content = t) for t in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [q.text for q in client.completions.create(prompt=inputs_mixtral_q, model=\"mixtral-instruct\", temperature=0.0, top_p=0.01, max_tokens=2048).choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_mixtral_a = [prompt_mixtral_a.format(context_str = t, query_str=q) for t,q in zip(texts, question)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [a.text for a in client.completions.create(prompt=inputs_mixtral_a, model=\"mixtral-instruct\", temperature=0.0, max_tokens=4096).choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## évaluation qualité de la génération \n",
    "\n",
    "Objectif vérifier que le dataset possède une vraie diversité pour éviter d'évaluer sur des cas d'usages trop uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "questions = df[\"eval_question\"].to_list()\n",
    "question_len = pd.DataFrame([len(q) for q in questions], columns=[\"length\"])\n",
    "question_len.hist(bins=50)\n",
    "plt.title(\"Histogram of Question Lengths\")\n",
    "plt.xlabel(\"Question Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "question_embeddings = client.embeddings.create(\n",
    "            input = questions,\n",
    "            model= \"dgfip-e5-large\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = [e.embedding for e in question_embeddings.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding = reducer.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    umap_embedding[:, 0],\n",
    "    umap_embedding[:, 1],)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the LLM generate question dataset', fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarité moyenne entre chunk et question générée\n",
    "\n",
    "chunk_embeddings = [e.embedding for e in client.embeddings.create(\n",
    "            input = df.text.to_list(),\n",
    "            model= \"dgfip-e5-large\"\n",
    "        ).data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_relevancy = []\n",
    "for i in tqdm(range(len(chunk_embeddings))):\n",
    "    cosim_relevancy.append(cossim(embs[i], chunk_embeddings[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer que la génération automatique de question via Mixtral est plutôt bonne, elle conduite à une bonne diversité thématique et les documents sont proches de leur chunk respectifs, ce qui est plutôt un bon point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex question generation \n",
    "\n",
    "Objectif : Généner une "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayd-api",
   "language": "python",
   "name": "ayd-api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
