{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from typing import List, Any\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "os.environ[\"no_proxy\"] = \"10.156.254.10\"\n",
    "import openai\n",
    "openai.api_key = \"dtnumds\"\n",
    "openai.api_base = \"http://10.156.254.10:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"dtnumds\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://10.156.254.10:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI( api_key=\"dtnumds\",\n",
    "                azure_endpoint=\"http://10.156.254.10:8000/v1\",\n",
    "                api_version = \"2023-07-01-preview\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in client.models.list().data :\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_embedding(docs: List[str], model: str) -> List[Any]:\n",
    "    \"\"\"This function computes embeddings for a given list of texts\n",
    "\n",
    "    Args:\n",
    "        docs (List[str]): Input texts to embed\n",
    "        model (str): Embedding model to used\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    embeddings = client.embeddings.create(model=model, input=docs)\n",
    "    return [e.embedding for e in embeddings.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"liste.de.mots.francais.frgut.txt\", 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict = data.split('\\n')\n",
    "import re\n",
    "def generate_random_word():\n",
    "    word = \"\"\n",
    "    while len(word) < 6:\n",
    "        word = fr_dict[random.randint(0,len(fr_dict))]\n",
    "    return re.sub('\\n', '', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic labelled generation \n",
    "generation_answer_pt = \"\"\"[INST]\n",
    "Ta mission est de généré des réponses de chat synthétiques en français. L'objectif est de pouvoir par la suite entrâiner un modèle.  Tu te baseras sur le mot de vocabulaire suivant :\n",
    "{random_word}\n",
    "Tu renverras des nouvelles réponses rédigées en français à partir du mots de vocabulaire \n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompts = [generation_answer_pt.format(random_word = generate_random_word()) for _ in range((N_EXAMPLE))]\n",
    "exemple_reponse = [r.text.split(\"\\n\")[1] for r in client.completions.create(prompt=prompts, model=\"mistral-7b-awq\", temperature=0.0, max_tokens=256).choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_no_context_prompt = \"\"\"\n",
    "Ta mission est de générer de maximum de formulation possible pour exprimer l'absence d'informations dans un context donnée.\n",
    "Par exemple :\n",
    "- Je ne dispose pas d'informations dans le contexte fourni pour répondre à la question concernant le diamètre de la Terre.\n",
    "- Le contexte suivante ne donne aucune information à propos de la taille de la ville de Paris. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_context_example = [c if len(c) > 10 else \"vide\" for c in client.completions.create(prompt=generate_no_context_prompt,model=\"mixtral-instruct\", temperature=0.0, max_tokens=32000).choices[0].text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_context_example += [c if len(c) > 10 else \"vide\" for c in client.completions.create(prompt=generate_no_context_prompt,model=\"mixtral-instruct\", temperature=0.1, max_tokens=4096).choices[0].text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "exemple_reponse_df = pd.DataFrame(exemple_reponse, columns = ['text'])\n",
    "exemple_reponse_df['label']  = 0\n",
    "exemple_no_context_df = pd.DataFrame(no_context_example, columns = ['text'])\n",
    "exemple_no_context_df['label'] = 1\n",
    "exemple_no_context_df = exemple_no_context_df.loc[exemple_no_context_df['text'] != \"vide\"]\n",
    "\n",
    "df = pd.concat((exemple_no_context_df, exemple_reponse_df)).reset_index(drop=True).sample( frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../data/train_data_clf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs  = compute_embedding(df['text'].to_list(), model='dgfip-e5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embs, df['label'], test_size=0.33, random_state=432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "clf = HistGradientBoostingClassifier(max_depth=3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skops.io import dump, load\n",
    "serialized = dump(clf, \"../../ai_models/clf_pr.skops\")\n",
    "loaded = load(\"../../ai_models/clf_pr.skops\", trusted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayd-api",
   "language": "python",
   "name": "ayd-api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
